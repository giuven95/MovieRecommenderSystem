{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm0moEr3vkHN"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle\n",
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU6Eh2rJwHgH",
        "outputId": "38dc86ac-a140-41d0-ab0e-73ad56843678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                              title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "ahsan81/hotel-reservations-classification-dataset                Hotel Reservations Dataset                          480KB  2023-01-04 12:50:31           7858        276  1.0              \n",
            "googleai/musiccaps                                               MusicCaps                                           793KB  2023-01-25 09:25:48           1236        136  0.9411765        \n",
            "themrityunjaypathak/most-subscribed-1000-youtube-channels        Most Subscribed 1000 Youtube Channels                28KB  2023-01-21 14:42:05           1678         56  1.0              \n",
            "senapatirajesh/netflix-tv-shows-and-movies                       Latest Netflix TV shows and movies                    1MB  2023-01-14 17:03:12           2953         80  0.9411765        \n",
            "ulrikthygepedersen/online-retail-dataset                         Online Retail Dataset                                 7MB  2023-01-20 13:50:59           1088         28  1.0              \n",
            "yanmaksi/big-startup-secsees-fail-dataset-from-crunchbase        ðŸš€Startup Success/Fail Dataset from Crunchbase         3MB  2023-01-22 19:14:28           1012         34  1.0              \n",
            "yashwanthkumarmn/motorcycles-in-india                            Motorcycles in India                                  9KB  2023-01-14 09:35:13           1221         26  1.0              \n",
            "salimwid/technology-company-layoffs-20222023-data                Technology Company Layoffs (2022-2023)               13KB  2023-01-24 07:07:51           1182         35  1.0              \n",
            "cesaber/spam-email-data-spamassassin-2002                        Spam Email Data original & CSV file (Spamassassin)   12MB  2023-01-24 09:16:05            439         28  1.0              \n",
            "ahbab911/top-250-korean-dramas-kdrama-dataset                    Top 250 Korean Dramas (KDrama) Dataset               90KB  2023-01-21 10:06:24            788         40  1.0              \n",
            "rakkesharv/spotify-top-10000-streamed-songs                      Spotify Top 10000 Streamed Songs                    280KB  2023-01-02 08:17:15           3055         89  1.0              \n",
            "rishikeshkonapure/home-loan-approval                             Home Loan Approval                                   13KB  2023-01-12 06:28:57           2375         55  1.0              \n",
            "themrityunjaypathak/imdb-top-100-movies                          IMDb Top 100 Movies                                   4KB  2023-01-11 17:15:09           1966         53  1.0              \n",
            "ahsan81/job-placement-dataset                                    Job Placement Dataset                                 4KB  2023-01-09 09:58:49           1432         34  1.0              \n",
            "yashsrivastava51213/revenue-and-profit-of-fortune-500-companies  Revenue and Profit of Fortune 500 Companies.        328KB  2023-01-22 05:46:13            603         28  0.88235295       \n",
            "thedevastator/rotten-tomatoes-top-movies-ratings-and-technical   Rotten Tomatoes Top Movies Ratings and Technical    685KB  2023-01-17 09:43:42            886         37  1.0              \n",
            "thedevastator/global-fossil-co2-emissions-by-country-2002-2022   Emissions by Country                                  2MB  2023-01-24 04:24:10           2844         83  1.0              \n",
            "thedevastator/supermarket-ordering-invoicing-and-sales-analysi   Supermarket Ordering, Invoicing, and Sales            4MB  2023-01-15 15:37:10           1195         34  0.9411765        \n",
            "shibumohapatra/customer-life-time-value                          Customer Life Time Value                              2MB  2023-01-23 09:52:06            535         26  1.0              \n",
            "karkavelrajaj/amazon-sales-dataset                               Amazon Sales Dataset                                  2MB  2023-01-17 06:21:15           1789         54  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Move the Kaggle API Token in the correct folder, test it works\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wue3pE0VxS-9",
        "outputId": "31afd1cd-0ef4-44e4-fc2c-b3c2cdd7dba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading letterboxd-movie-ratings-data.zip to /content\n",
            " 91% 171M/188M [00:02<00:00, 92.6MB/s]\n",
            "100% 188M/188M [00:02<00:00, 85.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Kaggle\n",
        "! kaggle datasets download samlearner/letterboxd-movie-ratings-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT-D7kmFxqNn",
        "outputId": "ad45e142-bacf-4c89-8120-6b2ffbbe5c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  letterboxd-movie-ratings-data.zip\n",
            "  inflating: dataset/movie_data.csv  \n",
            "  inflating: dataset/ratings_export.csv  \n",
            "  inflating: dataset/users_export.csv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the data\n",
        "! unzip letterboxd-movie-ratings-data.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPiCihnpyhti"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a Pandas dataframe\n",
        "movie_data = pd.read_csv(\"dataset/movie_data.csv\", lineterminator=\"\\n\")\n",
        "ratings_data = pd.read_csv(\"dataset/ratings_export.csv\", lineterminator=\"\\n\")\n",
        "user_data = pd.read_csv(\"dataset/users_export.csv\", lineterminator=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOne4YU4w89_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzvq5AgKDH8y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_and_save_or_load_maps(d):\n",
        "  mm = \"./processed/movie_to_index.pkl\"\n",
        "  uu = \"./processed/user_to_index.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"movie_to_index\"] = pickle.load(f)\n",
        "    with open(uu, 'rb') as f: d[\"user_to_index\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MAPS\")\n",
        "    d[\"movie_to_index\"] = {m: i for i, m in enumerate(d[\"all_movies\"])}\n",
        "    d[\"user_to_index\"] = {u: i for i, u in enumerate(d[\"all_users\"])}\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"movie_to_index\"], f)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"user_to_index\"], f)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgwRhG1ULjo8"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_movies(d):\n",
        "  mm = \"./processed/all_movies.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"all_movies\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MOVIES\")\n",
        "    d[\"all_movies\"] = ratings_data.movie_id.unique()\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"all_movies\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLDkeA82McYs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_and_save_or_load_users(d, train_ratio, test_ratio):\n",
        "  uu = \"./processed/all_users.pkl\"\n",
        "  tt1 = \"./processed/train_users.pkl\"\n",
        "  tt2 = \"./processed/test_users.pkl\"\n",
        "  try:\n",
        "    with open(uu, 'rb') as f: d[\"all_users\"] = pickle.load(f)\n",
        "    with open(tt1, 'rb') as f: d[\"train_users\"] = pickle.load(f)\n",
        "    with open(tt2, 'rb') as f: d[\"test_users\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD USERS\")\n",
        "    d[\"all_users\"] = ratings_data.user_id.unique()\n",
        "    d[\"train_users\"], d[\"test_users\"] = train_test_split(d[\"all_users\"], train_size=train_ratio, test_size=test_ratio)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"all_users\"], f)\n",
        "    with open(tt1, 'wb') as f: pickle.dump(d[\"train_users\"], f)\n",
        "    with open(tt2, 'wb') as f: pickle.dump(d[\"test_users\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGQvomU3R-nu"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def get_and_save_or_load_matrix(d, test_or_train):\n",
        "  s = test_or_train + \"_matrix\"\n",
        "  tt = \"./processed/\" + s + \".pkl\"\n",
        "  try:\n",
        "    with open(tt, 'rb') as f: d[s] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MATRIX: \" + test_or_train)\n",
        "    shape = (len(d[\"all_users\"]), len(d[\"all_movies\"]))\n",
        "    df = ratings_data[ratings_data[\"user_id\"].isin(d[test_or_train + \"_users\"])]\n",
        "    row = df['user_id'].map(d['user_to_index']).values\n",
        "    col = df['movie_id'].map(d['movie_to_index']).values\n",
        "    data = df['rating_val'].values\n",
        "    d[s] = coo_matrix((data, (row, col)), shape=shape)\n",
        "    d[s] = d[s].tocsr()\n",
        "    with open(tt, 'wb') as f: pickle.dump(d[s], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf3wb8f4qmLk"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_sample(train_ratio=0.8, test_ratio=0.2):\n",
        "  d = dict()\n",
        "  d = get_and_save_or_load_movies(d)\n",
        "  d = get_and_save_or_load_users(d, train_ratio, test_ratio)\n",
        "  d = get_and_save_or_load_maps(d)\n",
        "  for s in (\"test\", \"train\"):\n",
        "    d = get_and_save_or_load_matrix(d, s)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG_1dNRiMOc1",
        "outputId": "61b5915f-c05c-4659-e15e-5fa940d32ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  processed.zip\n",
            "   creating: ./content/processed/\n",
            "  inflating: ./content/processed/test_users.pkl  \n",
            "  inflating: ./content/processed/user_to_index.pkl  \n",
            "  inflating: ./content/processed/all_users.pkl  \n",
            "  inflating: ./content/processed/train_matrix.pkl  \n",
            "  inflating: ./content/processed/train_users.pkl  \n",
            "  inflating: ./content/processed/movie_to_index.pkl  \n",
            "  inflating: ./content/processed/test_matrix.pkl  \n",
            "  inflating: ./content/processed/all_movies.pkl  \n"
          ]
        }
      ],
      "source": [
        "# I added the processed.zip file manually\n",
        "! unzip processed.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1tO_DqyqRdE"
      },
      "outputs": [],
      "source": [
        "! mkdir /content/processed\n",
        "! mv /content/content/processed/* /content/processed/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOWAUe-Kqi_0"
      },
      "outputs": [],
      "source": [
        "! rm -rf /content/content\n",
        "! rm -rf /content/processed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Pv_-C_RDwC",
        "outputId": "a48f62f0-6779-4c84-aaaf-df771237ee54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-92-9c44d1654815>:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  averages = sums / counts\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 0.56193742,  0.56193742,  0.56193742, ...,  2.78800631,\n",
              "       -0.21199369, -1.21199369])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def subtract_column(sparse_matrix, column):\n",
        "    column = column.flatten()\n",
        "    nonzero_rows, nonzero_cols = sparse_matrix.nonzero()\n",
        "    nonzero_values = sparse_matrix.data\n",
        "    nonzero_values -= column[nonzero_rows]\n",
        "    new_sparse_matrix = sparse_matrix.copy()\n",
        "    new_sparse_matrix.data[:] = nonzero_values\n",
        "    return new_sparse_matrix\n",
        "\n",
        "def demean_matrix(mat):\n",
        "    sums = mat.sum(axis=1).A1\n",
        "    counts = np.diff(mat.indptr)\n",
        "    averages = sums / counts\n",
        "    averages = averages.reshape(-1, 1)\n",
        "    return subtract_column(mat, averages)\n",
        "\n",
        "d = get_and_save_or_load_sample()\n",
        "d[\"train_matrix_demeaned\"] = demean_matrix(d[\"train_matrix\"].asfptype())\n",
        "d[\"test_matrix_demeaned\"] = demean_matrix(d[\"test_matrix\"].asfptype())\n",
        "d[\"train_matrix_demeaned\"].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alEj82BXL130"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "LATENT_DIMENSION = 100\n",
        "DROPOUT_RATE = 0.5\n",
        "HIDDEN_LAYER_SIZE = 128\n",
        "\n",
        "def custom_final_activation(x):\n",
        "    return K.hard_sigmoid(x) * 10\n",
        "\n",
        "def build_model(num_movies):\n",
        "    single_movie_input = layers.Input(shape=(1,), name=\"single_movie\")\n",
        "    weighted_average_input = layers.Input(shape=(LATENT_DIMENSION,), name=\"weighted_average\")\n",
        "\n",
        "    movie_embedding_op = layers.Embedding(num_movies, LATENT_DIMENSION, name=\"movie_embedding_op\")\n",
        "    \n",
        "    single_movie_embedding = movie_embedding_op(single_movie_input)\n",
        "    single_movie_embedding_reshaped = layers.Reshape((LATENT_DIMENSION,), name=\"single_movie_embedding_reshaped\")(single_movie_embedding)\n",
        "\n",
        "    x = layers.Concatenate()([weighted_average_input, single_movie_embedding_reshaped])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    x = layers.Dense(1, activation=custom_final_activation, name=\"predicted_rating\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[single_movie_input, weighted_average_input], outputs=x), movie_embedding_op\n",
        "\n",
        "model, movie_embedding_op = build_model(d[\"train_matrix\"].shape[1])\n",
        "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X7Rx6NdP79D",
        "outputId": "2dfa16d4-c7f6-45e0-8a3b-4018163f2163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "([array([[10198]], dtype=int32), array([[    20,     25,     27, ..., 274868, 280831, 282677]], dtype=int32), array([[-3.87184343,  1.12815657, -5.87184343, ...,  2.12815657,\n",
            "         2.12815657,  0.12815657]])], array([8]))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_training_sample():\n",
        "    nonzero_indices = d[\"train_matrix\"].nonzero()\n",
        "    ui = np.random.choice(nonzero_indices[0])\n",
        "    sparse_row = d[\"train_matrix\"][ui]\n",
        "    _, movie_indices = sparse_row.nonzero()\n",
        "    mi = np.random.choice(movie_indices)\n",
        "\n",
        "    num_rated_movies = len(movie_indices)\n",
        "\n",
        "    single_movie_input = np.array([mi]).reshape(-1, 1)\n",
        "    multiple_movie_input = movie_indices.reshape(-1, num_rated_movies)\n",
        "    ratings_input = d[\"train_matrix_demeaned\"][ui, movie_indices].reshape(-1, num_rated_movies).toarray()\n",
        "    rating_to_predict = np.array([d[\"train_matrix\"][ui, mi]])\n",
        "\n",
        "    return [single_movie_input, multiple_movie_input, ratings_input], rating_to_predict\n",
        "\n",
        "print(extract_training_sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7afDIBLhKa-"
      },
      "outputs": [],
      "source": [
        "%load_ext cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI2_--j2tTwz"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "cimport numpy as np\n",
        "\n",
        "def cython_inner(DM, np.ndarray[object, ndim=1] multiple_movie_input, int i, int num_columns, np.ndarray[int, ndim=1] random_user_indices):\n",
        "    cdef int j\n",
        "    cdef np.ndarray[double, ndim=1] arr = np.zeros((num_columns,))\n",
        "    cdef int row_index = random_user_indices[i]\n",
        "    cdef int start = DM.indptr[row_index]\n",
        "    cdef int end = DM.indptr[row_index+1]\n",
        "    cdef np.ndarray[int, ndim=1] col_indices = DM.indices[start:end]\n",
        "    cdef np.ndarray[double, ndim=1] data = DM.data[start:end]\n",
        "    \n",
        "    for j in range(num_columns):\n",
        "        col = multiple_movie_input[i][j]\n",
        "        try:\n",
        "            index = np.where(col_indices == col)[0][0]\n",
        "            arr[j] = data[index]\n",
        "        except:\n",
        "            arr[j] = 0\n",
        "    return arr\n",
        "\n",
        "def cython_demeaned_rating_matrix(DM, np.ndarray[object, ndim=1] multiple_movie_input, \n",
        "                                  np.ndarray[int, ndim=1] random_user_indices, \n",
        "                                  int num_rows, np.ndarray[long, ndim=1] row_lengths):\n",
        "    cdef int i\n",
        "    cdef np.ndarray[object, ndim=1] demeaned_rating_matrix = np.empty((num_rows,), dtype='object')\n",
        "    for i in range(num_rows):\n",
        "        demeaned_rating_matrix[i] = cython_inner(DM, multiple_movie_input, i, row_lengths[i], random_user_indices)\n",
        "    \n",
        "    return demeaned_rating_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsTCMkIA84YY",
        "outputId": "ade355bf-1622-4080-c396-70384ab927ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([<tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              "  array([[  5669.],\n",
              "         [  3816.],\n",
              "         [  3141.],\n",
              "         [   593.],\n",
              "         [ 47417.],\n",
              "         [   337.],\n",
              "         [  1177.],\n",
              "         [  1685.],\n",
              "         [   722.],\n",
              "         [   923.],\n",
              "         [  8904.],\n",
              "         [ 17520.],\n",
              "         [ 52081.],\n",
              "         [ 24714.],\n",
              "         [  3964.],\n",
              "         [  8845.],\n",
              "         [ 19767.],\n",
              "         [  6527.],\n",
              "         [ 32796.],\n",
              "         [  6911.],\n",
              "         [ 28741.],\n",
              "         [ 59311.],\n",
              "         [  1743.],\n",
              "         [   455.],\n",
              "         [  5434.],\n",
              "         [ 22426.],\n",
              "         [ 16519.],\n",
              "         [  1761.],\n",
              "         [ 10392.],\n",
              "         [  7156.],\n",
              "         [  9185.],\n",
              "         [  3582.],\n",
              "         [ 39008.],\n",
              "         [  7606.],\n",
              "         [  8120.],\n",
              "         [255112.],\n",
              "         [  8215.],\n",
              "         [ 29863.],\n",
              "         [ 16653.],\n",
              "         [  4464.],\n",
              "         [190248.],\n",
              "         [  3594.],\n",
              "         [  2296.],\n",
              "         [  3998.],\n",
              "         [  4111.],\n",
              "         [  5859.],\n",
              "         [  2572.],\n",
              "         [ 17378.],\n",
              "         [139271.],\n",
              "         [  9605.],\n",
              "         [  2387.],\n",
              "         [  1907.],\n",
              "         [ 12864.],\n",
              "         [  5495.],\n",
              "         [  3961.],\n",
              "         [ 49583.],\n",
              "         [ 31674.],\n",
              "         [ 43921.],\n",
              "         [  1491.],\n",
              "         [ 14458.],\n",
              "         [ 50875.],\n",
              "         [ 18526.],\n",
              "         [  5037.],\n",
              "         [  1907.],\n",
              "         [   634.],\n",
              "         [   709.],\n",
              "         [   442.],\n",
              "         [214662.],\n",
              "         [  9449.],\n",
              "         [   946.],\n",
              "         [  5408.],\n",
              "         [  1605.],\n",
              "         [234664.],\n",
              "         [ 11254.],\n",
              "         [  7711.],\n",
              "         [  1267.],\n",
              "         [  8511.],\n",
              "         [  3035.],\n",
              "         [  4707.],\n",
              "         [  7694.],\n",
              "         [ 28064.],\n",
              "         [ 15844.],\n",
              "         [   599.],\n",
              "         [ 69107.],\n",
              "         [  1402.],\n",
              "         [  1455.],\n",
              "         [  5196.],\n",
              "         [ 17881.],\n",
              "         [  8350.],\n",
              "         [  4241.],\n",
              "         [  1494.],\n",
              "         [207555.],\n",
              "         [  5423.],\n",
              "         [162353.],\n",
              "         [ 39131.],\n",
              "         [ 15283.],\n",
              "         [  1543.],\n",
              "         [ 18324.],\n",
              "         [  2021.],\n",
              "         [  3538.]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(100, 100), dtype=float32, numpy=\n",
              "  array([[-0.00103457, -0.00182269, -0.00229515, ...,  0.00364741,\n",
              "           0.00261959,  0.00200363],\n",
              "         [-0.01042755, -0.01588263, -0.0182267 , ...,  0.02758188,\n",
              "           0.02235873,  0.01452174],\n",
              "         [-0.01070109, -0.01882342, -0.02211357, ...,  0.03350552,\n",
              "           0.02741936,  0.01804355],\n",
              "         ...,\n",
              "         [-0.00567723, -0.01011855, -0.01033981, ...,  0.01686069,\n",
              "           0.01333278,  0.00874099],\n",
              "         [-0.00235224, -0.00362139, -0.0039948 , ...,  0.00679449,\n",
              "           0.0051785 ,  0.00312975],\n",
              "         [-0.00688487, -0.01250039, -0.01361104, ...,  0.01955219,\n",
              "           0.0166618 ,  0.01153459]], dtype=float32)>],\n",
              " <tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              " array([[ 3.],\n",
              "        [ 7.],\n",
              "        [ 1.],\n",
              "        [ 4.],\n",
              "        [ 8.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 3.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 3.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 3.],\n",
              "        [ 2.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 5.],\n",
              "        [ 4.],\n",
              "        [ 9.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 3.],\n",
              "        [ 6.],\n",
              "        [ 2.],\n",
              "        [ 9.],\n",
              "        [ 5.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 5.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 9.],\n",
              "        [ 9.],\n",
              "        [ 7.],\n",
              "        [ 2.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 5.],\n",
              "        [ 5.],\n",
              "        [ 9.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 1.],\n",
              "        [ 7.],\n",
              "        [10.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 6.]], dtype=float32)>)"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def extract_training_batch(batch_size=100):\n",
        "    M = d[\"train_matrix\"]\n",
        "    DM = d[\"train_matrix_demeaned\"].copy()\n",
        "    DM[:, 0] = DM[:, 0] - DM[:, 0]\n",
        "    user_indices, movie_indices = M.nonzero()\n",
        "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
        "    random_user_indices = user_indices[index_indices]\n",
        "    random_movie_indices = movie_indices[index_indices]\n",
        "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
        "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
        "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
        "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
        "\n",
        "    def pad_array(array, max_len, padding_value=0.0):\n",
        "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
        "        return padded\n",
        "\n",
        "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
        "        max_len = max([len(x) for x in multiple_arrays])\n",
        "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
        "        return np.array(padded)\n",
        "\n",
        "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
        "    num_rows = len(multiple_movie_input)\n",
        "    num_columns = len(multiple_movie_input[0])\n",
        "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
        "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
        "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
        "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
        "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
        "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
        "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
        "    weighted_average_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
        "    normalized_weighted_average_input = tf.nn.l2_normalize(weighted_average_input)\n",
        "    X_train = [single_movie_input, normalized_weighted_average_input]\n",
        "    Y_train = ratings_to_predict.astype('float32')\n",
        "    Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "    return X_train, Y_train\n",
        "\n",
        "extract_training_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYPnA9KWWvXs",
        "outputId": "68f7f404-41e2-426a-f8f6-3ebecb361e11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " multiple_movies (InputLayer)   [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " ratings (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " movie_embedding_op (Embedding)  multiple            28607100    ['single_movie[0][0]',           \n",
            "                                                                  'multiple_movies[0][0]']        \n",
            "                                                                                                  \n",
            " single_movie (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " weighted_sum (Lambda)          (None, None, 100)    0           ['ratings[0][0]',                \n",
            "                                                                  'movie_embedding_op[1][0]']     \n",
            "                                                                                                  \n",
            " weighted_sum_reshaped (Reshape  (None, 100)         0           ['weighted_sum[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " single_movie_embedding_reshape  (None, 100)         0           ['movie_embedding_op[0][0]']     \n",
            " d (Reshape)                                                                                      \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 200)          0           ['weighted_sum_reshaped[0][0]',  \n",
            "                                                                  'single_movie_embedding_reshaped\n",
            "                                                                 [0][0]']                         \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 200)         800         ['concatenate[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          25728       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          16512       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " predicted_rating (Dense)       (None, 1)            129         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28,650,269\n",
            "Trainable params: 28,649,869\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuXumWhoTm4M",
        "outputId": "33729a94-2ae2-4bb2-f318-e1bf360d640c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 531ms/step - loss: 58.7177\n"
          ]
        }
      ],
      "source": [
        "single_movie_input, multiple_movie_input, ratings_input, rating_to_predict = extract_training_data()\n",
        "history = model.fit(\n",
        "    x=[single_movie_input, multiple_movie_input, ratings_input], \n",
        "    y=rating_to_predict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VnBPJpIRCuN",
        "outputId": "e862d236-bdad-43d1-d3fb-3c1eff102613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2860/2860 [==============================] - 5478s 2s/step - loss: 3.0167\n",
            "Epoch 2/10\n",
            "2860/2860 [==============================] - 5321s 2s/step - loss: 2.9508\n",
            "Epoch 3/10\n",
            "2860/2860 [==============================] - 5229s 2s/step - loss: 2.9048\n",
            "Epoch 4/10\n",
            "1956/2860 [===================>..........] - ETA: 28:48 - loss: 2.8844"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, batch_size=100):\n",
        "        self.num_movies = d[\"train_matrix\"].shape[1]\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(self.num_movies / self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return extract_training_batch(self.batch_size)\n",
        "\n",
        "training_generator = DataGenerator()\n",
        "history = model.fit(training_generator, epochs=NUM_EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VvPmMJiWtX1U",
        "outputId": "ab109433-8b8a-4a4d-ec77-91f3da2bd04c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}