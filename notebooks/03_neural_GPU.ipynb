{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Gm0moEr3vkHN",
        "outputId": "064dde9a-7373-4eac-d902-5527464ac180"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5c8ab0be-bea1-474c-a93b-b841bb977c90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5c8ab0be-bea1-474c-a93b-b841bb977c90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"giuseppevenuto\",\"key\":\"b789d1bcaee3e95873e6a9f9533a5ec3\"}'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install Kaggle\n",
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU6Eh2rJwHgH",
        "outputId": "8081ce7c-b888-40be-835e-21010b465ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                              title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "ahsan81/hotel-reservations-classification-dataset                Hotel Reservations Dataset                          480KB  2023-01-04 12:50:31           7897        278  1.0              \n",
            "googleai/musiccaps                                               MusicCaps                                           793KB  2023-01-25 09:25:48           1259        139  0.9411765        \n",
            "themrityunjaypathak/most-subscribed-1000-youtube-channels        Most Subscribed 1000 Youtube Channels                28KB  2023-01-21 14:42:05           1704         56  1.0              \n",
            "senapatirajesh/netflix-tv-shows-and-movies                       Latest Netflix TV shows and movies                    1MB  2023-01-14 17:03:12           2980         80  0.9411765        \n",
            "ulrikthygepedersen/online-retail-dataset                         Online Retail Dataset                                 7MB  2023-01-20 13:50:59           1095         28  1.0              \n",
            "yanmaksi/big-startup-secsees-fail-dataset-from-crunchbase        ðŸš€Startup Success/Fail Dataset from Crunchbase         3MB  2023-01-22 19:14:28           1027         34  1.0              \n",
            "yashwanthkumarmn/motorcycles-in-india                            Motorcycles in India                                  9KB  2023-01-14 09:35:13           1225         26  1.0              \n",
            "salimwid/technology-company-layoffs-20222023-data                Technology Company Layoffs (2022-2023)               13KB  2023-01-24 07:07:51           1197         35  1.0              \n",
            "cesaber/spam-email-data-spamassassin-2002                        Spam Email Data original & CSV file (Spamassassin)   12MB  2023-01-24 09:16:05            441         28  1.0              \n",
            "ahbab911/top-250-korean-dramas-kdrama-dataset                    Top 250 Korean Dramas (KDrama) Dataset               90KB  2023-01-21 10:06:24            790         40  1.0              \n",
            "rakkesharv/spotify-top-10000-streamed-songs                      Spotify Top 10000 Streamed Songs                    280KB  2023-01-02 08:17:15           3074         89  1.0              \n",
            "rishikeshkonapure/home-loan-approval                             Home Loan Approval                                   13KB  2023-01-12 06:28:57           2395         56  1.0              \n",
            "themrityunjaypathak/imdb-top-100-movies                          IMDb Top 100 Movies                                   4KB  2023-01-11 17:15:09           1973         53  1.0              \n",
            "ahsan81/job-placement-dataset                                    Job Placement Dataset                                 4KB  2023-01-09 09:58:49           1441         35  1.0              \n",
            "yashsrivastava51213/revenue-and-profit-of-fortune-500-companies  Revenue and Profit of Fortune 500 Companies.        328KB  2023-01-22 05:46:13            609         28  0.88235295       \n",
            "thedevastator/rotten-tomatoes-top-movies-ratings-and-technical   Rotten Tomatoes Top Movies Ratings and Technical    685KB  2023-01-17 09:43:42            891         37  1.0              \n",
            "thedevastator/global-fossil-co2-emissions-by-country-2002-2022   Emissions by Country                                  2MB  2023-01-24 04:24:10           2854         84  1.0              \n",
            "thedevastator/supermarket-ordering-invoicing-and-sales-analysi   Supermarket Ordering, Invoicing, and Sales            4MB  2023-01-15 15:37:10           1209         34  0.9411765        \n",
            "shibumohapatra/customer-life-time-value                          Customer Life Time Value                              2MB  2023-01-23 09:52:06            537         26  1.0              \n",
            "karkavelrajaj/amazon-sales-dataset                               Amazon Sales Dataset                                  2MB  2023-01-17 06:21:15           1809         54  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Move the Kaggle API Token in the correct folder, test it works\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wue3pE0VxS-9",
        "outputId": "406113f2-c681-4e08-f769-ad9ce042fee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading letterboxd-movie-ratings-data.zip to /content\n",
            " 97% 182M/188M [00:01<00:00, 148MB/s]\n",
            "100% 188M/188M [00:01<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Kaggle\n",
        "! kaggle datasets download samlearner/letterboxd-movie-ratings-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT-D7kmFxqNn",
        "outputId": "d56e749b-b1d4-454d-cb11-354ee2a1563b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  letterboxd-movie-ratings-data.zip\n",
            "  inflating: dataset/movie_data.csv  \n",
            "  inflating: dataset/ratings_export.csv  \n",
            "  inflating: dataset/users_export.csv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the data\n",
        "! unzip letterboxd-movie-ratings-data.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4PrEr6CF4J6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPiCihnpyhti"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a Pandas dataframe\n",
        "movie_data = pd.read_csv(\"dataset/movie_data.csv\", lineterminator=\"\\n\")\n",
        "ratings_data = pd.read_csv(\"dataset/ratings_export.csv\", lineterminator=\"\\n\")\n",
        "user_data = pd.read_csv(\"dataset/users_export.csv\", lineterminator=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzvq5AgKDH8y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_and_save_or_load_maps(d):\n",
        "  mm = \"./processed/movie_to_index.pkl\"\n",
        "  uu = \"./processed/user_to_index.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"movie_to_index\"] = pickle.load(f)\n",
        "    with open(uu, 'rb') as f: d[\"user_to_index\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MAPS\")\n",
        "    d[\"movie_to_index\"] = {m: i for i, m in enumerate(d[\"all_movies\"])}\n",
        "    d[\"user_to_index\"] = {u: i for i, u in enumerate(d[\"all_users\"])}\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"movie_to_index\"], f)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"user_to_index\"], f)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgwRhG1ULjo8"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_movies(d):\n",
        "  mm = \"./processed/all_movies.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"all_movies\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MOVIES\")\n",
        "    d[\"all_movies\"] = ratings_data.movie_id.unique()\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"all_movies\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLDkeA82McYs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_and_save_or_load_users(d, train_ratio, test_ratio):\n",
        "  uu = \"./processed/all_users.pkl\"\n",
        "  tt1 = \"./processed/train_users.pkl\"\n",
        "  tt2 = \"./processed/test_users.pkl\"\n",
        "  try:\n",
        "    with open(uu, 'rb') as f: d[\"all_users\"] = pickle.load(f)\n",
        "    with open(tt1, 'rb') as f: d[\"train_users\"] = pickle.load(f)\n",
        "    with open(tt2, 'rb') as f: d[\"test_users\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD USERS\")\n",
        "    d[\"all_users\"] = ratings_data.user_id.unique()\n",
        "    d[\"train_users\"], d[\"test_users\"] = train_test_split(d[\"all_users\"], train_size=train_ratio, test_size=test_ratio)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"all_users\"], f)\n",
        "    with open(tt1, 'wb') as f: pickle.dump(d[\"train_users\"], f)\n",
        "    with open(tt2, 'wb') as f: pickle.dump(d[\"test_users\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGQvomU3R-nu"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def get_and_save_or_load_matrix(d, test_or_train):\n",
        "  s = test_or_train + \"_matrix\"\n",
        "  tt = \"./processed/\" + s + \".pkl\"\n",
        "  try:\n",
        "    with open(tt, 'rb') as f: d[s] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MATRIX: \" + test_or_train)\n",
        "    shape = (len(d[\"all_users\"]), len(d[\"all_movies\"]))\n",
        "    df = ratings_data[ratings_data[\"user_id\"].isin(d[test_or_train + \"_users\"])]\n",
        "    row = df['user_id'].map(d['user_to_index']).values\n",
        "    col = df['movie_id'].map(d['movie_to_index']).values\n",
        "    data = df['rating_val'].values\n",
        "    d[s] = coo_matrix((data, (row, col)), shape=shape)\n",
        "    d[s] = d[s].tocsr()\n",
        "    with open(tt, 'wb') as f: pickle.dump(d[s], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf3wb8f4qmLk"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_sample(train_ratio=0.8, test_ratio=0.2):\n",
        "  d = dict()\n",
        "  d = get_and_save_or_load_movies(d)\n",
        "  d = get_and_save_or_load_users(d, train_ratio, test_ratio)\n",
        "  d = get_and_save_or_load_maps(d)\n",
        "  for s in (\"test\", \"train\"):\n",
        "    d = get_and_save_or_load_matrix(d, s)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3RvqY47KzB5"
      },
      "outputs": [],
      "source": [
        "! mkdir /content/processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Pb32IzKYGN",
        "outputId": "26bd9b58-208f-41dd-8c91-72ba7cf7f654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COULD NOT LOAD MOVIES\n",
            "COULD NOT LOAD USERS\n",
            "COULD NOT LOAD MAPS\n",
            "COULD NOT LOAD MATRIX: test\n",
            "COULD NOT LOAD MATRIX: train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-9c44d1654815>:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  averages = sums / counts\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-1.0555973 ,  2.9444027 , -2.0555973 , ...,  2.78800631,\n",
              "       -0.21199369, -1.21199369])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def subtract_column(sparse_matrix, column):\n",
        "    column = column.flatten()\n",
        "    nonzero_rows, nonzero_cols = sparse_matrix.nonzero()\n",
        "    nonzero_values = sparse_matrix.data\n",
        "    nonzero_values -= column[nonzero_rows]\n",
        "    new_sparse_matrix = sparse_matrix.copy()\n",
        "    new_sparse_matrix.data[:] = nonzero_values\n",
        "    return new_sparse_matrix\n",
        "\n",
        "def demean_matrix(mat):\n",
        "    sums = mat.sum(axis=1).A1\n",
        "    counts = np.diff(mat.indptr)\n",
        "    averages = sums / counts\n",
        "    averages = averages.reshape(-1, 1)\n",
        "    return subtract_column(mat, averages)\n",
        "\n",
        "d = get_and_save_or_load_sample()\n",
        "d[\"train_matrix_demeaned\"] = demean_matrix(d[\"train_matrix\"].asfptype())\n",
        "d[\"test_matrix_demeaned\"] = demean_matrix(d[\"test_matrix\"].asfptype())\n",
        "d[\"train_matrix_demeaned\"].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alEj82BXL130"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "LATENT_DIMENSION = 100\n",
        "DROPOUT_RATE = 0.5\n",
        "HIDDEN_LAYER_SIZE = 128\n",
        "\n",
        "def custom_final_activation(x):\n",
        "    return K.hard_sigmoid(x) * 10\n",
        "\n",
        "def build_model(num_movies):\n",
        "    single_movie_input = layers.Input(shape=(1,), name=\"single_movie\")\n",
        "    weighted_average_input = layers.Input(shape=(LATENT_DIMENSION,), name=\"weighted_average\")\n",
        "\n",
        "    movie_embedding_op = layers.Embedding(num_movies, LATENT_DIMENSION, name=\"movie_embedding_op\")\n",
        "    \n",
        "    single_movie_embedding = movie_embedding_op(single_movie_input)\n",
        "    single_movie_embedding_reshaped = layers.Reshape((LATENT_DIMENSION,), name=\"single_movie_embedding_reshaped\")(single_movie_embedding)\n",
        "\n",
        "    x = layers.Concatenate()([weighted_average_input, single_movie_embedding_reshaped])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    x = layers.Dense(1, activation=custom_final_activation, name=\"predicted_rating\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[single_movie_input, weighted_average_input], outputs=x), movie_embedding_op\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model, movie_embedding_op = build_model(d[\"train_matrix\"].shape[1])\n",
        "  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7afDIBLhKa-"
      },
      "outputs": [],
      "source": [
        "%load_ext cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsTCMkIA84YY",
        "outputId": "25f02a80-46db-44c4-ad7a-050e78ad8c40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([<tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              "  array([[6.98530e+04],\n",
              "         [1.24700e+03],\n",
              "         [3.58800e+03],\n",
              "         [4.17600e+03],\n",
              "         [4.75900e+03],\n",
              "         [1.42799e+05],\n",
              "         [3.46900e+03],\n",
              "         [5.42400e+03],\n",
              "         [2.47820e+04],\n",
              "         [1.61100e+03],\n",
              "         [2.96150e+04],\n",
              "         [4.86570e+04],\n",
              "         [3.95600e+03],\n",
              "         [2.45300e+03],\n",
              "         [7.22100e+03],\n",
              "         [3.10490e+04],\n",
              "         [4.51000e+02],\n",
              "         [1.02293e+05],\n",
              "         [5.12500e+03],\n",
              "         [1.78300e+03],\n",
              "         [1.24371e+05],\n",
              "         [2.64800e+03],\n",
              "         [1.07170e+04],\n",
              "         [4.10900e+03],\n",
              "         [5.52500e+03],\n",
              "         [1.07640e+04],\n",
              "         [4.94710e+04],\n",
              "         [3.26420e+04],\n",
              "         [2.91000e+02],\n",
              "         [3.12290e+04],\n",
              "         [5.02000e+02],\n",
              "         [2.45410e+04],\n",
              "         [1.31740e+04],\n",
              "         [1.76100e+03],\n",
              "         [1.49830e+04],\n",
              "         [2.86200e+03],\n",
              "         [2.60600e+03],\n",
              "         [6.12430e+04],\n",
              "         [1.92600e+03],\n",
              "         [2.92870e+04],\n",
              "         [2.00300e+03],\n",
              "         [2.54400e+03],\n",
              "         [1.05800e+03],\n",
              "         [2.93750e+04],\n",
              "         [2.67000e+02],\n",
              "         [3.34300e+03],\n",
              "         [6.84000e+02],\n",
              "         [4.61300e+03],\n",
              "         [6.80000e+01],\n",
              "         [3.39600e+03],\n",
              "         [1.89220e+04],\n",
              "         [4.36200e+03],\n",
              "         [7.74700e+03],\n",
              "         [8.93000e+03],\n",
              "         [3.64270e+04],\n",
              "         [1.98060e+04],\n",
              "         [2.81200e+03],\n",
              "         [5.35300e+03],\n",
              "         [1.55080e+04],\n",
              "         [1.95400e+03],\n",
              "         [3.48300e+03],\n",
              "         [1.56630e+04],\n",
              "         [1.59050e+04],\n",
              "         [5.89000e+02],\n",
              "         [7.48000e+02],\n",
              "         [6.28800e+03],\n",
              "         [3.21770e+04],\n",
              "         [7.04000e+02],\n",
              "         [1.97830e+04],\n",
              "         [3.86400e+03],\n",
              "         [4.79900e+03],\n",
              "         [2.79000e+02],\n",
              "         [1.72200e+03],\n",
              "         [1.34900e+03],\n",
              "         [1.75300e+03],\n",
              "         [5.29000e+02],\n",
              "         [5.23470e+04],\n",
              "         [2.02000e+03],\n",
              "         [3.55500e+03],\n",
              "         [4.63070e+04],\n",
              "         [3.49000e+02],\n",
              "         [3.43070e+04],\n",
              "         [2.03000e+02],\n",
              "         [1.59420e+05],\n",
              "         [8.64790e+04],\n",
              "         [5.27100e+03],\n",
              "         [1.91721e+05],\n",
              "         [3.23100e+03],\n",
              "         [1.77600e+03],\n",
              "         [1.16000e+02],\n",
              "         [1.32150e+04],\n",
              "         [4.49870e+04],\n",
              "         [4.56850e+04],\n",
              "         [1.29230e+04],\n",
              "         [2.92000e+03],\n",
              "         [1.73200e+03],\n",
              "         [2.96400e+03],\n",
              "         [2.63016e+05],\n",
              "         [8.70000e+03],\n",
              "         [2.39107e+05]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(100, 100), dtype=float32, numpy=\n",
              "  array([[ 0.00799462,  0.01100786, -0.00393834, ...,  0.00504285,\n",
              "          -0.01285028,  0.00607539],\n",
              "         [ 0.004619  ,  0.00802171, -0.00273289, ...,  0.00270065,\n",
              "          -0.00859209,  0.00346512],\n",
              "         [ 0.01687776,  0.02634216, -0.00983186, ...,  0.01226406,\n",
              "          -0.02889487,  0.01284055],\n",
              "         ...,\n",
              "         [ 0.00654084,  0.01087206, -0.00362526, ...,  0.00555778,\n",
              "          -0.01192097,  0.0046286 ],\n",
              "         [ 0.00493731,  0.0092029 , -0.00298301, ...,  0.00355066,\n",
              "          -0.00937815,  0.00461085],\n",
              "         [ 0.01279267,  0.01905477, -0.00627213, ...,  0.00986249,\n",
              "          -0.01908871,  0.00812625]], dtype=float32)>],\n",
              " <tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              " array([[ 6.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 3.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [10.],\n",
              "        [10.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [10.],\n",
              "        [ 8.],\n",
              "        [10.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 2.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 9.],\n",
              "        [ 7.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 2.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 9.],\n",
              "        [ 2.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [10.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 8.],\n",
              "        [ 9.],\n",
              "        [ 1.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 5.],\n",
              "        [ 3.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 3.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [10.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 7.],\n",
              "        [ 3.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 9.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 3.],\n",
              "        [ 6.],\n",
              "        [10.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 2.]], dtype=float32)>)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def extract_training_batch(batch_size=100):\n",
        "    M = d[\"train_matrix\"]\n",
        "    DM = d[\"train_matrix_demeaned\"].copy()\n",
        "    DM[:, 0] = DM[:, 0] - DM[:, 0]\n",
        "    user_indices, movie_indices = M.nonzero()\n",
        "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
        "    random_user_indices = user_indices[index_indices]\n",
        "    random_movie_indices = movie_indices[index_indices]\n",
        "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
        "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
        "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
        "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
        "\n",
        "    def pad_array(array, max_len, padding_value=0.0):\n",
        "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
        "        return padded\n",
        "\n",
        "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
        "        max_len = max([len(x) for x in multiple_arrays])\n",
        "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
        "        return np.array(padded)\n",
        "\n",
        "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
        "    num_rows = len(multiple_movie_input)\n",
        "    num_columns = len(multiple_movie_input[0])\n",
        "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
        "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
        "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
        "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
        "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
        "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
        "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
        "    weighted_average_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
        "    normalized_weighted_average_input = tf.nn.l2_normalize(weighted_average_input)\n",
        "    X_train = [single_movie_input, normalized_weighted_average_input]\n",
        "    Y_train = ratings_to_predict.astype('float32')\n",
        "    Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "    return X_train, Y_train\n",
        "\n",
        "extract_training_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VnBPJpIRCuN",
        "outputId": "b66f52c3-0fa5-4e23-c2f5-d7f6657178ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2860/2860 [==============================] - 4041s 1s/step - loss: 3.0110\n",
            "Epoch 2/10\n",
            "2860/2860 [==============================] - 4007s 1s/step - loss: 2.9500\n",
            "Epoch 3/10\n",
            "2860/2860 [==============================] - 3995s 1s/step - loss: 2.9129\n",
            "Epoch 4/10\n",
            " 395/2860 [===>..........................] - ETA: 57:52 - loss: 2.8688"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, batch_size=100):\n",
        "        self.num_movies = d[\"train_matrix\"].shape[1]\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(self.num_movies / self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return extract_training_batch(self.batch_size)\n",
        "\n",
        "training_generator = DataGenerator()\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(training_generator, epochs=NUM_EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOP5T6sOxS_w",
        "outputId": "9044a88d-cd30-4177-b31f-f3dec1981f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 37s 366ms/step - loss: 3.1639\n"
          ]
        }
      ],
      "source": [
        "def extract_evaluation_batch(batch_size=100):\n",
        "    M = d[\"test_matrix\"]\n",
        "    DM = d[\"test_matrix_demeaned\"]\n",
        "    user_indices, movie_indices = M.nonzero()\n",
        "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
        "    random_user_indices = user_indices[index_indices]\n",
        "    random_movie_indices = movie_indices[index_indices]\n",
        "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
        "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
        "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
        "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
        "\n",
        "    def pad_array(array, max_len, padding_value=0.0):\n",
        "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
        "        return padded\n",
        "\n",
        "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
        "        max_len = max([len(x) for x in multiple_arrays])\n",
        "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
        "        return np.array(padded)\n",
        "\n",
        "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
        "    num_rows = len(multiple_movie_input)\n",
        "    num_columns = len(multiple_movie_input[0])\n",
        "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
        "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
        "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
        "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
        "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
        "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
        "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
        "    weighted_average_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
        "    X_test = [single_movie_input, weighted_average_input]\n",
        "    Y_test = ratings_to_predict.astype('float32')\n",
        "    Y_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
        "    return X_test, Y_test\n",
        "\n",
        "class EvaluationDataGenerator(Sequence):\n",
        "    def __init__(self, num_samples, batch_size=100):\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(self.num_samples / self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return extract_evaluation_batch(self.batch_size)\n",
        "\n",
        "evaluation_generator = EvaluationDataGenerator(10000)\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model.evaluate(evaluation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_mvpmqf0kxH",
        "outputId": "633fbd2c-5c52-4e56-e949-0e55256c6b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array(['rti03'], dtype=object),\n",
              " array(['frankenstein-1931'], dtype=object),\n",
              " array([[5.]], dtype=float32),\n",
              " array([[6.873782]], dtype=float32))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def extract_prediction_batch(batch_size=100):\n",
        "    M = d[\"test_matrix\"]\n",
        "    DM = d[\"test_matrix_demeaned\"]\n",
        "    user_indices, movie_indices = M.nonzero()\n",
        "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
        "    random_user_indices = user_indices[index_indices]\n",
        "    random_movie_indices = movie_indices[index_indices]\n",
        "    random_user_names = d[\"all_users\"][random_user_indices]\n",
        "    random_movie_names = d[\"all_movies\"][random_movie_indices]\n",
        "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
        "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
        "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
        "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
        "\n",
        "    def pad_array(array, max_len, padding_value=0.0):\n",
        "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
        "        return padded\n",
        "\n",
        "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
        "        max_len = max([len(x) for x in multiple_arrays])\n",
        "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
        "        return np.array(padded)\n",
        "\n",
        "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
        "    num_rows = len(multiple_movie_input)\n",
        "    num_columns = len(multiple_movie_input[0])\n",
        "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
        "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
        "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
        "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
        "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
        "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
        "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
        "    weighted_average_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
        "    X_test = [single_movie_input, weighted_average_input]\n",
        "    Y_test = ratings_to_predict = ratings_to_predict.astype('float32')\n",
        "    Y_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
        "    return X_test, Y_test, ratings_to_predict, random_user_names, random_movie_names\n",
        "\n",
        "X_predict, Y_predict, ratings_to_predict, random_user_names, random_movie_names = extract_prediction_batch(1)\n",
        "random_user_names, random_movie_names, ratings_to_predict, model.predict(X_predict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}