{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZTNF4XZKvTK",
        "outputId": "59460a08-f21b-4a86-ace3-9f3065b98a64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Gm0moEr3vkHN",
        "outputId": "1f8110ef-0236-48fe-bd89-c5c9abf1495f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8330a4b-ff35-41f5-900a-e83e3562a6f9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8330a4b-ff35-41f5-900a-e83e3562a6f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"giuseppevenuto\",\"key\":\"b789d1bcaee3e95873e6a9f9533a5ec3\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Install Kaggle\n",
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU6Eh2rJwHgH",
        "outputId": "ec48dcb2-9235-40a1-e60f-f8a59ec98e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "ahsan81/hotel-reservations-classification-dataset               Hotel Reservations Dataset                         480KB  2023-01-04 12:50:31           8316        288  1.0              \n",
            "googleai/musiccaps                                              MusicCaps                                          793KB  2023-01-25 09:25:48           1509        161  0.9411765        \n",
            "themrityunjaypathak/most-subscribed-1000-youtube-channels       Most Subscribed 1000 Youtube Channels               28KB  2023-01-21 14:42:05           1965         63  1.0              \n",
            "nitishsharma01/olympics-124-years-datasettill-2020              Olympics 124 years Dataset(till 2020)                5MB  2023-02-01 09:00:49            677         24  1.0              \n",
            "thedevastator/medical-student-mental-health                     Medical Student Mental Health                       19KB  2023-01-25 01:00:14            831         30  1.0              \n",
            "senapatirajesh/netflix-tv-shows-and-movies                      Latest Netflix TV shows and movies                   1MB  2023-01-14 17:03:12           3191         87  0.9411765        \n",
            "thedevastator/predicting-credit-card-customer-attrition-with-m  Predicting Credit Card Customer Segmentation       379KB  2023-01-15 15:27:38            965         38  1.0              \n",
            "kane6543/most-watched-stocks-of-past-decade20132023             Most Watched Stocks of Past Decade(2013-2023)        2MB  2023-01-30 15:12:25            713         31  1.0              \n",
            "karkavelrajaj/amazon-sales-dataset                              Amazon Sales Dataset                                 2MB  2023-01-17 06:21:15           2046         56  1.0              \n",
            "thedevastator/us-film-industry-top-movies-directors             US Film Industry Top Movies & Directors            217KB  2023-01-22 08:57:44            473         23  1.0              \n",
            "salimwid/latest-top-3000-companies-ceo-salary-202223            CEO vs Worker Pay in Top 3000 US Companies [2023]  183KB  2023-02-02 09:44:56            291         24  1.0              \n",
            "tymekurban/new-cars-usa-202223-dataset                          New Cars USA 2022/23 dataset                       110KB  2023-01-19 12:09:19            646         24  1.0              \n",
            "abhishek14398/salary-dataset-simple-linear-regression           Salary Dataset - Simple linear regression           457B  2023-01-10 03:55:40           1748         46  1.0              \n",
            "salimwid/technology-company-layoffs-20222023-data               Technology Company Layoffs (2022-2023)              13KB  2023-01-24 07:07:51           1304         37  1.0              \n",
            "thedevastator/higher-education-predictors-of-student-retention  Predict students' dropout and academic success      87KB  2023-01-03 09:18:54           2499         69  1.0              \n",
            "rhugvedbhojane/fifa-world-cup-2022-players-statistics           FIFA World Cup 2022 Players Statistics              98KB  2023-01-27 20:27:08            910         27  0.8235294        \n",
            "karimabdulnabi/car-price                                        Car Price                                           50KB  2023-01-28 23:09:39            549         21  1.0              \n",
            "rishikeshkonapure/home-loan-approval                            Home Loan Approval                                  13KB  2023-01-12 06:28:57           2506         59  1.0              \n",
            "rakkesharv/spotify-top-10000-streamed-songs                     Spotify Top 10000 Streamed Songs                   280KB  2023-01-02 08:17:15           3144         91  1.0              \n",
            "thedevastator/canine-intelligence-and-size                      Dogs Intelligence and Size                           4KB  2023-01-21 22:09:31            953         45  0.9411765        \n"
          ]
        }
      ],
      "source": [
        "# Move the Kaggle API Token in the correct folder, test it works\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wue3pE0VxS-9",
        "outputId": "f364f0dd-45d2-458e-eb65-f0ae7180efc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading letterboxd-movie-ratings-data.zip to /content\n",
            " 94% 177M/188M [00:02<00:00, 68.7MB/s]\n",
            "100% 188M/188M [00:02<00:00, 83.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Kaggle\n",
        "! kaggle datasets download samlearner/letterboxd-movie-ratings-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT-D7kmFxqNn",
        "outputId": "62504388-49ce-407b-f285-87ea1497a8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  letterboxd-movie-ratings-data.zip\n",
            "  inflating: dataset/movie_data.csv  \n",
            "  inflating: dataset/ratings_export.csv  \n",
            "  inflating: dataset/users_export.csv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the data\n",
        "! unzip letterboxd-movie-ratings-data.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X4PrEr6CF4J6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uPiCihnpyhti"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a Pandas dataframe\n",
        "movie_data = pd.read_csv(\"dataset/movie_data.csv\", lineterminator=\"\\n\")\n",
        "ratings_data = pd.read_csv(\"dataset/ratings_export.csv\", lineterminator=\"\\n\")\n",
        "user_data = pd.read_csv(\"dataset/users_export.csv\", lineterminator=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lzvq5AgKDH8y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_and_save_or_load_maps(d):\n",
        "  mm = \"./processed/movie_to_index.pkl\"\n",
        "  uu = \"./processed/user_to_index.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"movie_to_index\"] = pickle.load(f)\n",
        "    with open(uu, 'rb') as f: d[\"user_to_index\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MAPS\")\n",
        "    d[\"movie_to_index\"] = {m: i for i, m in enumerate(d[\"all_movies\"])}\n",
        "    d[\"user_to_index\"] = {u: i for i, u in enumerate(d[\"all_users\"])}\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"movie_to_index\"], f)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"user_to_index\"], f)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IgwRhG1ULjo8"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_movies(d):\n",
        "  mm = \"./processed/all_movies.pkl\"\n",
        "  try:\n",
        "    with open(mm, 'rb') as f: d[\"all_movies\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MOVIES\")\n",
        "    d[\"all_movies\"] = ratings_data.movie_id.unique()\n",
        "    with open(mm, 'wb') as f: pickle.dump(d[\"all_movies\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pLDkeA82McYs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_and_save_or_load_users(d, train_ratio, test_ratio):\n",
        "  uu = \"./processed/all_users.pkl\"\n",
        "  tt1 = \"./processed/train_users.pkl\"\n",
        "  tt2 = \"./processed/test_users.pkl\"\n",
        "  try:\n",
        "    with open(uu, 'rb') as f: d[\"all_users\"] = pickle.load(f)\n",
        "    with open(tt1, 'rb') as f: d[\"train_users\"] = pickle.load(f)\n",
        "    with open(tt2, 'rb') as f: d[\"test_users\"] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD USERS\")\n",
        "    d[\"all_users\"] = ratings_data.user_id.unique()\n",
        "    d[\"train_users\"], d[\"test_users\"] = train_test_split(d[\"all_users\"], train_size=train_ratio, test_size=test_ratio)\n",
        "    with open(uu, 'wb') as f: pickle.dump(d[\"all_users\"], f)\n",
        "    with open(tt1, 'wb') as f: pickle.dump(d[\"train_users\"], f)\n",
        "    with open(tt2, 'wb') as f: pickle.dump(d[\"test_users\"], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AGQvomU3R-nu"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def get_and_save_or_load_matrix(d, test_or_train):\n",
        "  s = test_or_train + \"_matrix\"\n",
        "  tt = \"./processed/\" + s + \".pkl\"\n",
        "  try:\n",
        "    with open(tt, 'rb') as f: d[s] = pickle.load(f)\n",
        "  except:\n",
        "    print(\"COULD NOT LOAD MATRIX: \" + test_or_train)\n",
        "    shape = (len(d[\"all_users\"]), len(d[\"all_movies\"]))\n",
        "    df = ratings_data[ratings_data[\"user_id\"].isin(d[test_or_train + \"_users\"])]\n",
        "    row = df['user_id'].map(d['user_to_index']).values\n",
        "    col = df['movie_id'].map(d['movie_to_index']).values\n",
        "    data = df['rating_val'].values\n",
        "    d[s] = coo_matrix((data, (row, col)), shape=shape)\n",
        "    d[s] = d[s].tocsr()\n",
        "    with open(tt, 'wb') as f: pickle.dump(d[s], f)\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gf3wb8f4qmLk"
      },
      "outputs": [],
      "source": [
        "def get_and_save_or_load_sample(train_ratio=0.8, test_ratio=0.2):\n",
        "  d = dict()\n",
        "  d = get_and_save_or_load_movies(d)\n",
        "  d = get_and_save_or_load_users(d, train_ratio, test_ratio)\n",
        "  d = get_and_save_or_load_maps(d)\n",
        "  for s in (\"test\", \"train\"):\n",
        "    d = get_and_save_or_load_matrix(d, s)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y3RvqY47KzB5"
      },
      "outputs": [],
      "source": [
        "! mkdir -p /content/processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Pb32IzKYGN",
        "outputId": "b1e7b8bb-5627-40dc-a8d4-79fbc271d1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COULD NOT LOAD MOVIES\n",
            "COULD NOT LOAD USERS\n",
            "COULD NOT LOAD MAPS\n",
            "COULD NOT LOAD MATRIX: test\n",
            "COULD NOT LOAD MATRIX: train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-9c44d1654815>:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  averages = sums / counts\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.0555973 ,  2.9444027 , -2.0555973 , ...,  2.78800631,\n",
              "       -0.21199369, -1.21199369])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "def subtract_column(sparse_matrix, column):\n",
        "    column = column.flatten()\n",
        "    nonzero_rows, nonzero_cols = sparse_matrix.nonzero()\n",
        "    nonzero_values = sparse_matrix.data\n",
        "    nonzero_values -= column[nonzero_rows]\n",
        "    new_sparse_matrix = sparse_matrix.copy()\n",
        "    new_sparse_matrix.data[:] = nonzero_values\n",
        "    return new_sparse_matrix\n",
        "\n",
        "def demean_matrix(mat):\n",
        "    sums = mat.sum(axis=1).A1\n",
        "    counts = np.diff(mat.indptr)\n",
        "    averages = sums / counts\n",
        "    averages = averages.reshape(-1, 1)\n",
        "    return subtract_column(mat, averages)\n",
        "\n",
        "d = get_and_save_or_load_sample()\n",
        "d[\"train_matrix_demeaned\"] = demean_matrix(d[\"train_matrix\"].asfptype())\n",
        "d[\"test_matrix_demeaned\"] = demean_matrix(d[\"test_matrix\"].asfptype())\n",
        "d[\"train_matrix_demeaned\"].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "alEj82BXL130"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "LATENT_DIMENSION = 100\n",
        "DROPOUT_RATE = 0.5\n",
        "HIDDEN_LAYER_SIZE = 128\n",
        "\n",
        "def custom_final_activation(x):\n",
        "    return K.hard_sigmoid(x) * 10\n",
        "\n",
        "def build_model(num_movies):\n",
        "    single_movie_input = layers.Input(shape=(1,), name=\"single_movie\")\n",
        "    weighted_sum_input = layers.Input(shape=(LATENT_DIMENSION,), name=\"weighted_sum\")\n",
        "\n",
        "    movie_embedding_op = layers.Embedding(num_movies, LATENT_DIMENSION, name=\"movie_embedding_op\")\n",
        "    \n",
        "    single_movie_embedding = movie_embedding_op(single_movie_input)\n",
        "    single_movie_embedding_reshaped = layers.Reshape((LATENT_DIMENSION,), name=\"single_movie_embedding_reshaped\")(single_movie_embedding)\n",
        "\n",
        "    x = layers.Concatenate(name=\"concatenation\")([weighted_sum_input, single_movie_embedding_reshaped])\n",
        "    x = layers.BatchNormalization(name=\"bn1\")(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, name=\"d1\", activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE, name=\"dr1\")(x)\n",
        "    x = layers.Dense(HIDDEN_LAYER_SIZE, name=\"d2\", activation=\"selu\")(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE, name=\"dr2\")(x)\n",
        "    x = layers.Dense(1, activation=custom_final_activation, name=\"predicted_rating\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[single_movie_input, weighted_sum_input], outputs=x), movie_embedding_op\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model, movie_embedding_op = build_model(d[\"train_matrix\"].shape[1])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsTCMkIA84YY",
        "outputId": "89bf9073-0531-495c-af97-3d3f5c01ae19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              "  array([[1.70800e+03],\n",
              "         [1.90700e+03],\n",
              "         [8.37100e+03],\n",
              "         [8.49200e+03],\n",
              "         [2.24910e+04],\n",
              "         [1.29347e+05],\n",
              "         [1.71296e+05],\n",
              "         [1.32900e+03],\n",
              "         [1.93700e+03],\n",
              "         [4.60300e+03],\n",
              "         [4.68000e+02],\n",
              "         [3.76100e+03],\n",
              "         [3.51400e+03],\n",
              "         [4.48910e+04],\n",
              "         [7.51500e+03],\n",
              "         [1.22500e+03],\n",
              "         [3.04990e+04],\n",
              "         [8.81800e+03],\n",
              "         [2.12200e+03],\n",
              "         [3.39600e+03],\n",
              "         [7.69800e+03],\n",
              "         [1.38490e+04],\n",
              "         [4.16800e+03],\n",
              "         [5.06300e+03],\n",
              "         [5.98000e+02],\n",
              "         [3.26730e+04],\n",
              "         [2.52173e+05],\n",
              "         [4.19100e+03],\n",
              "         [3.56160e+04],\n",
              "         [4.43310e+04],\n",
              "         [6.74450e+04],\n",
              "         [4.37600e+03],\n",
              "         [4.59100e+03],\n",
              "         [1.63710e+04],\n",
              "         [5.08360e+04],\n",
              "         [6.63920e+04],\n",
              "         [2.87700e+03],\n",
              "         [3.51250e+04],\n",
              "         [8.33600e+03],\n",
              "         [3.07900e+03],\n",
              "         [1.62600e+03],\n",
              "         [2.80000e+02],\n",
              "         [6.00000e+02],\n",
              "         [5.52050e+04],\n",
              "         [6.49950e+04],\n",
              "         [2.13529e+05],\n",
              "         [1.15765e+05],\n",
              "         [2.16000e+02],\n",
              "         [5.85000e+02],\n",
              "         [2.82649e+05],\n",
              "         [3.23820e+04],\n",
              "         [1.95190e+04],\n",
              "         [5.02900e+03],\n",
              "         [8.56800e+03],\n",
              "         [1.81900e+03],\n",
              "         [5.08500e+03],\n",
              "         [3.22050e+04],\n",
              "         [1.03556e+05],\n",
              "         [1.50500e+04],\n",
              "         [8.40300e+03],\n",
              "         [2.07673e+05],\n",
              "         [1.77000e+03],\n",
              "         [8.43200e+03],\n",
              "         [1.59000e+02],\n",
              "         [4.25500e+03],\n",
              "         [7.38760e+04],\n",
              "         [2.65940e+04],\n",
              "         [5.69030e+04],\n",
              "         [4.03210e+04],\n",
              "         [2.58000e+03],\n",
              "         [5.37500e+03],\n",
              "         [1.19500e+03],\n",
              "         [4.25660e+04],\n",
              "         [5.54300e+03],\n",
              "         [5.40400e+03],\n",
              "         [2.78700e+04],\n",
              "         [9.09000e+02],\n",
              "         [5.37700e+03],\n",
              "         [2.49680e+04],\n",
              "         [7.83870e+04],\n",
              "         [1.45450e+04],\n",
              "         [2.69000e+02],\n",
              "         [4.57340e+04],\n",
              "         [1.39770e+04],\n",
              "         [2.20900e+03],\n",
              "         [6.31830e+04],\n",
              "         [1.51430e+04],\n",
              "         [8.73560e+04],\n",
              "         [2.40000e+02],\n",
              "         [1.70100e+03],\n",
              "         [1.62100e+03],\n",
              "         [2.21830e+05],\n",
              "         [1.88100e+03],\n",
              "         [1.74350e+04],\n",
              "         [7.85400e+03],\n",
              "         [2.06360e+04],\n",
              "         [2.92730e+04],\n",
              "         [9.58900e+03],\n",
              "         [1.59700e+03],\n",
              "         [3.85100e+03]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(100, 100), dtype=float32, numpy=\n",
              "  array([[ 0.00149084,  0.00473839, -0.00695396, ..., -0.00070357,\n",
              "           0.00200802, -0.01450197],\n",
              "         [-0.00424584,  0.00116241, -0.00067545, ..., -0.01308355,\n",
              "           0.00010843,  0.00469654],\n",
              "         [ 0.00257248,  0.00605094, -0.0100814 , ..., -0.01810465,\n",
              "           0.01392988, -0.01624369],\n",
              "         ...,\n",
              "         [-0.00098855, -0.00399651,  0.00156986, ..., -0.00514831,\n",
              "           0.00019995,  0.00855296],\n",
              "         [-0.01074993, -0.0011148 , -0.00208411, ...,  0.00546606,\n",
              "          -0.00247069, -0.00674807],\n",
              "         [-0.00786756,  0.03713407, -0.00213477, ..., -0.00758461,\n",
              "          -0.00113602,  0.00491957]], dtype=float32)>],\n",
              " <tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              " array([[ 7.],\n",
              "        [ 7.],\n",
              "        [10.],\n",
              "        [ 3.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 9.],\n",
              "        [ 5.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 2.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [10.],\n",
              "        [ 7.],\n",
              "        [ 9.],\n",
              "        [ 7.],\n",
              "        [10.],\n",
              "        [ 1.],\n",
              "        [ 6.],\n",
              "        [ 1.],\n",
              "        [ 2.],\n",
              "        [ 4.],\n",
              "        [ 7.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 3.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 2.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [10.],\n",
              "        [ 5.],\n",
              "        [ 8.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [10.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 5.],\n",
              "        [ 7.],\n",
              "        [ 5.],\n",
              "        [10.],\n",
              "        [ 4.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 1.],\n",
              "        [ 8.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 7.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 8.],\n",
              "        [ 1.],\n",
              "        [ 4.],\n",
              "        [ 8.],\n",
              "        [ 4.],\n",
              "        [ 1.],\n",
              "        [ 8.],\n",
              "        [ 6.],\n",
              "        [ 5.],\n",
              "        [ 9.],\n",
              "        [ 6.],\n",
              "        [ 7.],\n",
              "        [ 4.],\n",
              "        [ 8.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def extract_batch(mode, batch_size=100):\n",
        "    if (mode in \"training\", \"train\"):\n",
        "        M = d[\"train_matrix\"]\n",
        "        DM = d[\"train_matrix_demeaned\"].copy()\n",
        "    else:\n",
        "        M = d[\"test_matrix\"]\n",
        "        DM = d[\"test_matrix_demeaned\"].copy()\n",
        "    DM[:, 0] = DM[:, 0] - DM[:, 0]\n",
        "    user_indices, movie_indices = M.nonzero()\n",
        "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
        "    random_user_indices = user_indices[index_indices]\n",
        "    random_movie_indices = movie_indices[index_indices]\n",
        "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
        "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
        "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
        "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
        "\n",
        "    def pad_array(array, max_len, padding_value=0.0):\n",
        "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
        "        return padded\n",
        "\n",
        "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
        "        max_len = max([len(x) for x in multiple_arrays])\n",
        "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
        "        return np.array(padded)\n",
        "\n",
        "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
        "    num_rows = len(multiple_movie_input)\n",
        "    num_columns = len(multiple_movie_input[0])\n",
        "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
        "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
        "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
        "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
        "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
        "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
        "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
        "    weighted_sum_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
        "    normalized_weighted_sum_input = tf.nn.l2_normalize(weighted_sum_input)\n",
        "    X = [single_movie_input, normalized_weighted_sum_input]\n",
        "    Y = ratings_to_predict.astype('float32')\n",
        "    Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "    if mode not in (\"prediction\",): return X, Y\n",
        "    else: return X, Y, ratings_to_predict, random_user_names, random_movie_names\n",
        "\n",
        "extract_batch(\"training\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/models"
      ],
      "metadata": {
        "id": "jyJHo23RH9Kp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOP5T6sOxS_w",
        "outputId": "5aae7c64-a991-431e-a82c-c72c57d8644f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 56s 1s/step - loss: 6.5310 - mae: 2.1695\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import Sequence\n",
        "\n",
        "NUM_EVALUATION_SAMPLES = 5000\n",
        "\n",
        "class EvaluationDataGenerator(Sequence):\n",
        "    def __init__(self, num_samples, batch_size=100):\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(self.num_samples / self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return extract_batch(\"evaluation\", self.batch_size)\n",
        "\n",
        "evaluation_generator = EvaluationDataGenerator(NUM_EVALUATION_SAMPLES)\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model.evaluate(evaluation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VnBPJpIRCuN",
        "outputId": "1d3eca3c-0262-4fe3-c2d1-c6ed00df3a4a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8705 - mae: 1.3223\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7977 - mae: 1.3033\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.8456 - mae: 1.3130\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.8984 - mae: 1.3281\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.8963 - mae: 1.3163\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.8529 - mae: 1.3170\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7730 - mae: 1.2891\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.9008 - mae: 1.3227\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.8921 - mae: 1.3228\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.8518 - mae: 1.3152\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f130a32bd00> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 56s 1s/step - loss: 2.6282 - mae: 1.2603\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8803 - mae: 1.3306\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7932 - mae: 1.3033\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.8746 - mae: 1.3220\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8300 - mae: 1.3076\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8230 - mae: 1.3097\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8008 - mae: 1.2938\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7921 - mae: 1.3060\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.8387 - mae: 1.3146\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 169s 1s/step - loss: 2.7919 - mae: 1.3020\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.8227 - mae: 1.3125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f11f221ccd0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.7746 - mae: 1.2919\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8427 - mae: 1.3129\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7788 - mae: 1.3001\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.8327 - mae: 1.3096\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8049 - mae: 1.3071\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7817 - mae: 1.2996\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7347 - mae: 1.2836\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8570 - mae: 1.3134\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8182 - mae: 1.3056\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8743 - mae: 1.3225\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8441 - mae: 1.3159\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f11f0b535e0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 54s 1s/step - loss: 2.6277 - mae: 1.2667\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7900 - mae: 1.3010\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8186 - mae: 1.3080\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.8419 - mae: 1.3157\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7921 - mae: 1.2972\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8250 - mae: 1.3110\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7982 - mae: 1.3096\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8186 - mae: 1.3096\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7853 - mae: 1.3014\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8097 - mae: 1.3038\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7814 - mae: 1.2959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f11f0a3cbb0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 56s 1s/step - loss: 2.6091 - mae: 1.2470\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7485 - mae: 1.2912\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7818 - mae: 1.2974\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7759 - mae: 1.2956\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8166 - mae: 1.3039\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8364 - mae: 1.3138\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8196 - mae: 1.3068\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8425 - mae: 1.3123\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.8411 - mae: 1.3128\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7380 - mae: 1.2864\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.8187 - mae: 1.3050\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16ba560040> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.7599 - mae: 1.2798\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.8010 - mae: 1.2985\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7463 - mae: 1.2871\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.7977 - mae: 1.3049\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8245 - mae: 1.3048\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7853 - mae: 1.2993\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8077 - mae: 1.3066\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7655 - mae: 1.2934\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7051 - mae: 1.2822\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8572 - mae: 1.3163\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7565 - mae: 1.2920\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16c6359940> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 54s 1s/step - loss: 2.5863 - mae: 1.2496\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.7380 - mae: 1.2934\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7579 - mae: 1.2947\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.6865 - mae: 1.2717\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7779 - mae: 1.3014\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7657 - mae: 1.2925\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.8625 - mae: 1.3186\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.7424 - mae: 1.2913\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7744 - mae: 1.2917\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7941 - mae: 1.3032\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7803 - mae: 1.3027\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f176e472250> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.6237 - mae: 1.2439\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7796 - mae: 1.2995\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7605 - mae: 1.2948\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7933 - mae: 1.3012\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7588 - mae: 1.2953\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8064 - mae: 1.3057\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7584 - mae: 1.2937\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7380 - mae: 1.2837\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8051 - mae: 1.3067\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8157 - mae: 1.3046\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7993 - mae: 1.3027\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16c696ecd0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 54s 1s/step - loss: 2.6121 - mae: 1.2526\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7906 - mae: 1.2935\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7903 - mae: 1.2988\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7768 - mae: 1.2937\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7425 - mae: 1.2860\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7498 - mae: 1.2843\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 171s 1s/step - loss: 2.7702 - mae: 1.2958\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7391 - mae: 1.2842\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7356 - mae: 1.2895\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8004 - mae: 1.3044\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7690 - mae: 1.2964\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f13b71eb2b0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.7068 - mae: 1.2936\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7360 - mae: 1.2877\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.8302 - mae: 1.3065\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7022 - mae: 1.2775\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8112 - mae: 1.3061\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7369 - mae: 1.2811\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7321 - mae: 1.2845\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7474 - mae: 1.2867\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7527 - mae: 1.2923\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7816 - mae: 1.2946\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8027 - mae: 1.3072\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16c7ae1520> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.6129 - mae: 1.2612\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.6828 - mae: 1.2762\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8043 - mae: 1.3037\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7619 - mae: 1.2936\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7434 - mae: 1.2899\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.7104 - mae: 1.2818\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7528 - mae: 1.2911\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7787 - mae: 1.3017\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7376 - mae: 1.2844\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7652 - mae: 1.2953\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7938 - mae: 1.3062\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f130a55e7f0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.5599 - mae: 1.2426\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7464 - mae: 1.2894\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7646 - mae: 1.2907\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8109 - mae: 1.3034\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7544 - mae: 1.2916\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.7861 - mae: 1.2958\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7553 - mae: 1.2895\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7924 - mae: 1.2967\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7513 - mae: 1.2879\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7303 - mae: 1.2950\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7640 - mae: 1.2893\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16ba0d07f0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.6423 - mae: 1.2509\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7337 - mae: 1.2867\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7085 - mae: 1.2793\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7689 - mae: 1.2973\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7820 - mae: 1.2909\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 167s 1s/step - loss: 2.7718 - mae: 1.2949\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7690 - mae: 1.2886\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7274 - mae: 1.2800\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7287 - mae: 1.2860\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.6939 - mae: 1.2785\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7380 - mae: 1.2905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f13b71112b0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 54s 1s/step - loss: 2.6099 - mae: 1.2545\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7296 - mae: 1.2853\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7901 - mae: 1.2962\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7656 - mae: 1.2900\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7497 - mae: 1.2887\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7518 - mae: 1.2879\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7265 - mae: 1.2855\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7475 - mae: 1.2866\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 165s 1s/step - loss: 2.7713 - mae: 1.2995\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7455 - mae: 1.2894\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 166s 1s/step - loss: 2.7375 - mae: 1.2891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f130aba6f10> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 54s 1s/step - loss: 2.6551 - mae: 1.2613\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.8089 - mae: 1.3026\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7521 - mae: 1.2847\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.6689 - mae: 1.2697\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7674 - mae: 1.2958\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 164s 1s/step - loss: 2.7343 - mae: 1.2821\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 162s 1s/step - loss: 2.7683 - mae: 1.2889\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.7221 - mae: 1.2802\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 163s 1s/step - loss: 2.6611 - mae: 1.2692\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 173s 1s/step - loss: 2.7489 - mae: 1.2880\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 168s 1s/step - loss: 2.6984 - mae: 1.2779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16bac4bd30> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION\n",
            "50/50 [==============================] - 55s 1s/step - loss: 2.6642 - mae: 1.2688\n",
            "TRAINING\n",
            "Epoch 1/10\n",
            "108/150 [====================>.........] - ETA: 47s - loss: 2.6822 - mae: 1.2685"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from keras.utils import Sequence\n",
        "from google.colab import files\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "NUM_ITERATIONS = 1000\n",
        "NUM_EPOCHS_PER_ITERATION = 10\n",
        "NUM_SAMPLES_PER_EPOCH = 15000\n",
        "SAVED_MODEL_PATH = \"/content/models/checkpoint\"\n",
        "ZIP_PATHNAME = \"/content/drive/My Drive/letterboxd_neural_model_checkpoint/letterboxd_neural_model_checkpoint.zip\"\n",
        "\n",
        "class TrainingDataGenerator(Sequence):\n",
        "    def __init__(self, batch_size=BATCH_SIZE):\n",
        "        self.num_movies = NUM_SAMPLES_PER_EPOCH\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(self.num_movies / self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return extract_batch(\"training\", self.batch_size)\n",
        "\n",
        "training_generator = TrainingDataGenerator(batch_size=BATCH_SIZE)\n",
        "try:\n",
        "    model = load_model(SAVED_MODEL_PATH)\n",
        "except:\n",
        "    model.save(SAVED_MODEL_PATH)\n",
        "with tf.device('/device:GPU:0'):\n",
        "    for i in range(NUM_ITERATIONS):\n",
        "        print(\"TRAINING\")\n",
        "        model.fit(training_generator, epochs=NUM_EPOCHS_PER_ITERATION, verbose=1)\n",
        "        model.save(SAVED_MODEL_PATH)\n",
        "        shutil.make_archive(ZIP_PATHNAME[:-4], 'zip', SAVED_MODEL_PATH)\n",
        "        model = load_model(SAVED_MODEL_PATH)\n",
        "        print(\"EVALUATION\")\n",
        "        model.evaluate(evaluation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(ZIP_PATHNAME[:-4], 'zip', SAVED_MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jdCE5dXaqj6_",
        "outputId": "a7f46527-6180-45fc-e24d-da78a4d9761e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/letterboxd_neural_model_checkpoint.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(SAVED_MODEL_PATH)\n",
        "with open(SAVED_MODEL_PATH + \"/\" + HISTORY_TABLE_FILENAME, 'wb') as file_pi:\n",
        "    pickle.dump(history_table, file_pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcpo9jblqakA",
        "outputId": "61c576ee-6c43-405c-d4ea-f513a3eb35b6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16c6427a60> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f176e1a3b20> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16ca2f3fa0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16c61655b0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16baee9970> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16bac33c10> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16ba9aaf10> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f16b86e69d0> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.saving.saved_model.load.Dense object at 0x7f176fec3970> has the same name 'Dense' as a built-in Keras object. Consider renaming <class 'keras.saving.saved_model.load.Dense'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_mvpmqf0kxH",
        "outputId": "633fbd2c-5c52-4e56-e949-0e55256c6b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array(['rti03'], dtype=object),\n",
              " array(['frankenstein-1931'], dtype=object),\n",
              " array([[5.]], dtype=float32),\n",
              " array([[6.873782]], dtype=float32))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_predict, Y_predict, ratings_to_predict, random_user_names, random_movie_names = extract_prediction_batch(1)\n",
        "random_user_names, random_movie_names, ratings_to_predict, model.predict(X_predict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}