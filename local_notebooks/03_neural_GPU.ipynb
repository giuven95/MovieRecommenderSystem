{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab636bcc-6c3e-464f-9fa1-90a9a17cfa22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 600 /home/giuven/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa72b4a3-149d-4831-bed7-99e46562e683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183f8846-17a5-4d0d-a3b1-289eec893866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                              title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "---------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "ahsan81/hotel-reservations-classification-dataset                Hotel Reservations Dataset                          480KB  2023-01-04 12:50:31           8002        280  1.0              \n",
      "googleai/musiccaps                                               MusicCaps                                           793KB  2023-01-25 09:25:48           1302        143  0.9411765        \n",
      "themrityunjaypathak/most-subscribed-1000-youtube-channels        Most Subscribed 1000 Youtube Channels                28KB  2023-01-21 14:42:05           1763         57  1.0              \n",
      "senapatirajesh/netflix-tv-shows-and-movies                       Latest Netflix TV shows and movies                    1MB  2023-01-14 17:03:12           3014         80  0.9411765        \n",
      "ulrikthygepedersen/online-retail-dataset                         Online Retail Dataset                                 7MB  2023-01-20 13:50:59           1112         28  1.0              \n",
      "yanmaksi/big-startup-secsees-fail-dataset-from-crunchbase        ðŸš€Startup Success/Fail Dataset from Crunchbase         3MB  2023-01-22 19:14:28           1044         35  1.0              \n",
      "yashwanthkumarmn/motorcycles-in-india                            Motorcycles in India                                  9KB  2023-01-14 09:35:13           1244         26  1.0              \n",
      "salimwid/technology-company-layoffs-20222023-data                Technology Company Layoffs (2022-2023)               13KB  2023-01-24 07:07:51           1225         36  1.0              \n",
      "cesaber/spam-email-data-spamassassin-2002                        Spam Email Data original & CSV file (Spamassassin)   12MB  2023-01-24 09:16:05            446         28  1.0              \n",
      "ahbab911/top-250-korean-dramas-kdrama-dataset                    Top 250 Korean Dramas (KDrama) Dataset               90KB  2023-01-21 10:06:24            801         41  1.0              \n",
      "rakkesharv/spotify-top-10000-streamed-songs                      Spotify Top 10000 Streamed Songs                    280KB  2023-01-02 08:17:15           3091         89  1.0              \n",
      "rishikeshkonapure/home-loan-approval                             Home Loan Approval                                   13KB  2023-01-12 06:28:57           2427         58  1.0              \n",
      "themrityunjaypathak/imdb-top-100-movies                          IMDb Top 100 Movies                                   4KB  2023-01-11 17:15:09           1984         53  1.0              \n",
      "ahsan81/job-placement-dataset                                    Job Placement Dataset                                 4KB  2023-01-09 09:58:49           1461         35  1.0              \n",
      "yashsrivastava51213/revenue-and-profit-of-fortune-500-companies  Revenue and Profit of Fortune 500 Companies.        328KB  2023-01-22 05:46:13            611         29  0.88235295       \n",
      "thedevastator/rotten-tomatoes-top-movies-ratings-and-technical   Rotten Tomatoes Top Movies Ratings and Technical    685KB  2023-01-17 09:43:42            895         37  1.0              \n",
      "thedevastator/global-fossil-co2-emissions-by-country-2002-2022   Emissions by Country                                  2MB  2023-01-24 04:24:10           2873         85  1.0              \n",
      "thedevastator/supermarket-ordering-invoicing-and-sales-analysi   Supermarket Ordering, Invoicing, and Sales            4MB  2023-01-15 15:37:10           1229         35  0.9411765        \n",
      "shibumohapatra/customer-life-time-value                          Customer Life Time Value                              2MB  2023-01-23 09:52:06            541         26  1.0              \n",
      "karkavelrajaj/amazon-sales-dataset                               Amazon Sales Dataset                                  2MB  2023-01-17 06:21:15           1850         55  1.0              \n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba179432-be14-4455-a444-3ff2975570f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading letterboxd-movie-ratings-data.zip to /home/giuven\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188M/188M [04:58<00:00, 636kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188M/188M [04:58<00:00, 659kB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download samlearner/letterboxd-movie-ratings-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb4a829-33e8-4305-a07f-f74e11533be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  letterboxd-movie-ratings-data.zip\n",
      "  inflating: /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/dataset/movie_data.csv  \n",
      "  inflating: /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/dataset/ratings_export.csv  \n",
      "  inflating: /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/dataset/users_export.csv  \n"
     ]
    }
   ],
   "source": [
    "! mkdir /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/dataset\n",
    "! unzip letterboxd-movie-ratings-data.zip -d /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ae09b4-5bd7-424e-9aed-c0d3c73bf91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q numpy\n",
    "! pip install -q pandas\n",
    "\n",
    "NOTEBOOK_FOLDER = \"/home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "movie_data = pd.read_csv(NOTEBOOK_FOLDER + \"dataset/movie_data.csv\", lineterminator=\"\\n\")\n",
    "ratings_data = pd.read_csv(NOTEBOOK_FOLDER + \"dataset/ratings_export.csv\", lineterminator=\"\\n\")\n",
    "user_data = pd.read_csv(NOTEBOOK_FOLDER + \"dataset/users_export.csv\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a31b06-6131-4d27-8f06-6d85b1052df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /home/giuven/Desktop/MovieRecommenderSystem/local_notebooks/processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daa651d1-35a8-4542-8308-599cb3cb6b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_and_save_or_load_maps(d):\n",
    "    mm = NOTEBOOK_FOLDER + \"processed/movie_to_index.pkl\"\n",
    "    uu = NOTEBOOK_FOLDER + \"processed/user_to_index.pkl\"\n",
    "    try:\n",
    "        with open(mm, 'rb') as f: d[\"movie_to_index\"] = pickle.load(f)\n",
    "        with open(uu, 'rb') as f: d[\"user_to_index\"] = pickle.load(f)\n",
    "    except:\n",
    "        print(\"COULD NOT LOAD MAPS\")\n",
    "        d[\"movie_to_index\"] = {m: i for i, m in enumerate(d[\"all_movies\"])}\n",
    "        d[\"user_to_index\"] = {u: i for i, u in enumerate(d[\"all_users\"])}\n",
    "        with open(mm, 'wb') as f: pickle.dump(d[\"movie_to_index\"], f)\n",
    "        with open(uu, 'wb') as f: pickle.dump(d[\"user_to_index\"], f)\n",
    "    return d\n",
    "\n",
    "def get_and_save_or_load_movies(d):\n",
    "    mm = NOTEBOOK_FOLDER + \"processed/all_movies.pkl\"\n",
    "    try:\n",
    "        with open(mm, 'rb') as f: d[\"all_movies\"] = pickle.load(f)\n",
    "    except:\n",
    "        print(\"COULD NOT LOAD MOVIES\")\n",
    "        d[\"all_movies\"] = ratings_data.movie_id.unique()\n",
    "        with open(mm, 'wb') as f: pickle.dump(d[\"all_movies\"], f)\n",
    "    return d\n",
    "\n",
    "def get_and_save_or_load_users(d, train_ratio, test_ratio):\n",
    "    uu = NOTEBOOK_FOLDER + \"processed/all_users.pkl\"\n",
    "    tt1 = NOTEBOOK_FOLDER + \"processed/train_users.pkl\"\n",
    "    tt2 = NOTEBOOK_FOLDER + \"processed/test_users.pkl\"\n",
    "    try:\n",
    "        with open(uu, 'rb') as f: d[\"all_users\"] = pickle.load(f)\n",
    "        with open(tt1, 'rb') as f: d[\"train_users\"] = pickle.load(f)\n",
    "        with open(tt2, 'rb') as f: d[\"test_users\"] = pickle.load(f)\n",
    "    except:\n",
    "        print(\"COULD NOT LOAD USERS\")\n",
    "        d[\"all_users\"] = ratings_data.user_id.unique()\n",
    "        d[\"train_users\"], d[\"test_users\"] = train_test_split(d[\"all_users\"], train_size=train_ratio, test_size=test_ratio)\n",
    "        with open(uu, 'wb') as f: pickle.dump(d[\"all_users\"], f)\n",
    "        with open(tt1, 'wb') as f: pickle.dump(d[\"train_users\"], f)\n",
    "        with open(tt2, 'wb') as f: pickle.dump(d[\"test_users\"], f)\n",
    "    return d\n",
    "\n",
    "def get_and_save_or_load_matrix(d, test_or_train):\n",
    "    s = test_or_train + \"_matrix\"\n",
    "    tt = NOTEBOOK_FOLDER + \"processed/\" + s + \".pkl\"\n",
    "    try:\n",
    "        with open(tt, 'rb') as f: d[s] = pickle.load(f)\n",
    "    except:\n",
    "        print(\"COULD NOT LOAD MATRIX: \" + test_or_train)\n",
    "        shape = (len(d[\"all_users\"]), len(d[\"all_movies\"]))\n",
    "        df = ratings_data[ratings_data[\"user_id\"].isin(d[test_or_train + \"_users\"])]\n",
    "        row = df['user_id'].map(d['user_to_index']).values\n",
    "        col = df['movie_id'].map(d['movie_to_index']).values\n",
    "        data = df['rating_val'].values\n",
    "        d[s] = coo_matrix((data, (row, col)), shape=shape)\n",
    "        d[s] = d[s].tocsr()\n",
    "        with open(tt, 'wb') as f: pickle.dump(d[s], f)\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_and_save_or_load_sample(train_ratio=0.8, test_ratio=0.2):\n",
    "    d = dict()\n",
    "    d = get_and_save_or_load_movies(d)\n",
    "    d = get_and_save_or_load_users(d, train_ratio, test_ratio)\n",
    "    d = get_and_save_or_load_maps(d)\n",
    "    for s in (\"test\", \"train\"):\n",
    "        d = get_and_save_or_load_matrix(d, s)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bb5eb15-6a51-4e05-95ca-4fa1d17c2b12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COULD NOT LOAD MOVIES\n",
      "COULD NOT LOAD USERS\n",
      "COULD NOT LOAD MAPS\n",
      "COULD NOT LOAD MATRIX: test\n",
      "COULD NOT LOAD MATRIX: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1458218/2972115933.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  averages = sums / counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.0555973 ,  2.9444027 , -2.0555973 , ...,  2.78800631,\n",
       "       -0.21199369, -1.21199369])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtract_column(sparse_matrix, column):\n",
    "    column = column.flatten()\n",
    "    nonzero_rows, nonzero_cols = sparse_matrix.nonzero()\n",
    "    nonzero_values = sparse_matrix.data\n",
    "    nonzero_values -= column[nonzero_rows]\n",
    "    new_sparse_matrix = sparse_matrix.copy()\n",
    "    new_sparse_matrix.data[:] = nonzero_values\n",
    "    return new_sparse_matrix\n",
    "\n",
    "def demean_matrix(mat):\n",
    "    sums = mat.sum(axis=1).A1\n",
    "    counts = np.diff(mat.indptr)\n",
    "    averages = sums / counts\n",
    "    averages = averages.reshape(-1, 1)\n",
    "    return subtract_column(mat, averages)\n",
    "\n",
    "d = get_and_save_or_load_sample()\n",
    "d[\"train_matrix_demeaned\"] = demean_matrix(d[\"train_matrix\"].asfptype())\n",
    "d[\"test_matrix_demeaned\"] = demean_matrix(d[\"test_matrix\"].asfptype())\n",
    "d[\"train_matrix_demeaned\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50051570-4ccb-4107-b19d-56280316f847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q scikit-learn\n",
    "! pip install -q matplotlib\n",
    "! pip install -q scipy\n",
    "! pip install -q tensorflow\n",
    "! pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92a53175-7b79-43c3-94a9-e06cec175d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "260e936a-2fa5-431a-ab09-05ec2e41d678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 09:59:49.824469: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 114428400 exceeds 10% of free system memory.\n",
      "2023-02-04 09:59:49.852806: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 114428400 exceeds 10% of free system memory.\n",
      "2023-02-04 09:59:49.863890: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 114428400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "LATENT_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.5\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "def custom_final_activation(x):\n",
    "    return K.hard_sigmoid(x) * 10\n",
    "\n",
    "def build_model(num_movies):\n",
    "    single_movie_input = layers.Input(shape=(1,), name=\"single_movie\")\n",
    "    weighted_average_input = layers.Input(shape=(LATENT_DIMENSION,), name=\"weighted_average\")\n",
    "\n",
    "    movie_embedding_op = layers.Embedding(num_movies, LATENT_DIMENSION, name=\"movie_embedding_op\")\n",
    "    \n",
    "    single_movie_embedding = movie_embedding_op(single_movie_input)\n",
    "    single_movie_embedding_reshaped = layers.Reshape((LATENT_DIMENSION,), name=\"single_movie_embedding_reshaped\")(single_movie_embedding)\n",
    "\n",
    "    x = layers.Concatenate()([weighted_average_input, single_movie_embedding_reshaped])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
    "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
    "    x = layers.Dense(HIDDEN_LAYER_SIZE, activation=\"selu\")(x)\n",
    "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
    "    x = layers.Dense(1, activation=custom_final_activation, name=\"predicted_rating\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[single_movie_input, weighted_average_input], outputs=x), movie_embedding_op\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model, movie_embedding_op = build_model(d[\"train_matrix\"].shape[1])\n",
    "  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "617cb997-f656-4f5a-b571-b4dc407e0f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 09:59:41.804880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-04 09:59:41.804900: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 09:59:41.804915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (giuven-OMEN-Laptop-15-ek0xxx): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 09:59:41.805312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'movie_embedding_op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m extract_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 39\u001b[0m, in \u001b[0;36mextract_batch\u001b[0;34m(mode, batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m multiple_movie_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(multiple_movie_input, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     38\u001b[0m demeaned_rating_matrix \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(demeaned_rating_matrix, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 39\u001b[0m movie_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmovie_embedding_op\u001b[49m(multiple_movie_input)\n\u001b[1;32m     40\u001b[0m weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(demeaned_rating_matrix, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m weighted_average_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mmultiply(movie_embeddings, weights), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movie_embedding_op' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "\n",
    "def extract_batch(mode, batch_size=100):\n",
    "    if mode in (\"training, train\"):\n",
    "        M = d[\"train_matrix\"]\n",
    "        DM = d[\"train_matrix_demeaned\"].copy()\n",
    "    else:\n",
    "        M = d[\"test_matrix\"]\n",
    "        DM = d[\"test_matrix_demeaned\"].copy()\n",
    "    DM[:, 0] = DM[:, 0] - DM[:, 0]\n",
    "    user_indices, movie_indices = M.nonzero()\n",
    "    index_indices = np.random.choice(range(len(user_indices)), size=batch_size)\n",
    "    random_user_indices = user_indices[index_indices]\n",
    "    random_movie_indices = movie_indices[index_indices]\n",
    "    nonzero_indices_by_row = np.array(np.split(M.indices, M.indptr)[1:-1], dtype='object')\n",
    "    single_movie_input = np.array(random_movie_indices).reshape(batch_size, 1)\n",
    "    multiple_movie_input = nonzero_indices_by_row[random_user_indices]\n",
    "    ratings_to_predict = np.array([M.asfptype()[random_user_indices, random_movie_indices]]).reshape(batch_size, 1)\n",
    "\n",
    "    def pad_array(array, max_len, padding_value=0.0):\n",
    "        padded = np.pad(array, (0, max_len - len(array)), mode='constant', constant_values=padding_value)\n",
    "        return padded\n",
    "\n",
    "    def pad_multiple_arrays(multiple_arrays, padding_value=0.0):\n",
    "        max_len = max([len(x) for x in multiple_arrays])\n",
    "        padded = [pad_array(x, max_len, padding_value) for x in multiple_arrays]\n",
    "        return np.array(padded)\n",
    "\n",
    "    multiple_movie_input = pad_multiple_arrays(multiple_movie_input, padding_value=0)\n",
    "    num_rows = len(multiple_movie_input)\n",
    "    num_columns = len(multiple_movie_input[0])\n",
    "    repeated_random_user_indices = np.array([[random_user_indices[i]] * num_columns for i in range(num_rows)])\n",
    "    demeaned_rating_matrix = DM[repeated_random_user_indices, multiple_movie_input].toarray()\n",
    "    single_movie_input = tf.convert_to_tensor(single_movie_input, dtype=tf.float32)\n",
    "    multiple_movie_input = tf.convert_to_tensor(multiple_movie_input, dtype=tf.float32)\n",
    "    demeaned_rating_matrix = tf.convert_to_tensor(demeaned_rating_matrix, dtype=tf.float32)\n",
    "    movie_embeddings = movie_embedding_op(multiple_movie_input)\n",
    "    weights = tf.expand_dims(demeaned_rating_matrix, axis=-1)\n",
    "    weighted_average_input = tf.reduce_sum(tf.multiply(movie_embeddings, weights), axis=1)\n",
    "    normalized_weighted_average_input = tf.nn.l2_normalize(weighted_average_input)\n",
    "    X = [single_movie_input, normalized_weighted_average_input]\n",
    "    Y = ratings_to_predict.astype('float32')\n",
    "    Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "    if mode not in (\"prediction\",): return X, Y\n",
    "    else: return X, Y, ratings_to_predict, random_user_names, random_movie_names\n",
    "\n",
    "\n",
    "class TrainingDataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=100):\n",
    "        self.num_samples = d[\"train_matrix\"].shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(self.num_samples / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return extract_batch(\"training\", self.batch_size)\n",
    "\n",
    "\n",
    "class EvaluationDataGenerator(Sequence):\n",
    "    def __init__(self, num_samples, batch_size=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(self.num_samples / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return extract_batch(\"evaluation\", self.batch_size)\n",
    "\n",
    "\n",
    "extract_batch(\"training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
